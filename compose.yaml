services:
  qwen3-tts:
    build: .
    image: qwen3-tts-local:cuda124
    container_name: qwen3_tts
    restart: unless-stopped
    ports:
      - "8101:8000"
    environment:
      - MODEL_ID=Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - REQUEST_TIMEOUT=300
      - IDLE_TIMEOUT=120        # 0 = never unload (always-on mode)
      - AUDIO_CACHE_MAX=256
      - TORCH_COMPILE=true
      - VAD_TRIM=true
      - TEXT_NORMALIZE=true
      - VOICE_CACHE_MAX=32
      - PRELOAD_MODEL=false     # true = load model at startup instead of first request
    volumes:
      - ./models:/root/.cache/huggingface
    ipc: host  # Share host IPC namespace â€” reduces CUDA IPC overhead for GPU tensor sharing
    shm_size: "1g"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Increase if PRELOAD_MODEL=true (model load takes 10-15s)
