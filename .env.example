# ===========================================================================
# Qwen3-TTS Configuration
# Copy to .env and customize as needed.
# ===========================================================================

# --- Model ---

# HuggingFace model ID (downloaded to ./models on first run)
MODEL_ID=Qwen/Qwen3-TTS-12Hz-0.6B-Base

# Load model at startup instead of on first request (true/false)
PRELOAD_MODEL=true

# Quantization mode: "" (none), "fp8" (torchao, recommended), "int8" (bitsandbytes, currently broken)
QUANTIZE=fp8

# --- Performance ---

# Enable torch.compile on the model (true/false)
TORCH_COMPILE=true

# torch.compile mode: "max-autotune" (slower warmup, fastest steady-state),
# "reduce-overhead", or "default"
TORCH_COMPILE_MODE=max-autotune

# Enable CUDA graph capture via triton backend (true/false)
CUDA_GRAPHS=true

# Seconds of inactivity before GPU model is unloaded (0 = never unload)
IDLE_TIMEOUT=0

# Max seconds per inference request before 504 timeout
REQUEST_TIMEOUT=300

# Max concurrent requests in queue before 503 rejection (0 = unlimited)
MAX_QUEUE_DEPTH=5

# Max jobs per GPU batch dispatch (1 = disable batching)
MAX_BATCH_SIZE=4

# --- Audio ---

# Max cached audio outputs, keyed by (text, voice, speed, format) hash (0 = disable cache)
AUDIO_CACHE_MAX=256

# Max cached voice clone speaker embeddings (0 = disable cache)
VOICE_CACHE_MAX=32

# Expand numbers, currency symbols, and abbreviations before synthesis (true/false)
TEXT_NORMALIZE=true

# --- Logging ---

# Log output format: "json" (structured) or "text" (human-readable)
LOG_FORMAT=json

# Minimum log level: DEBUG, INFO, SUCCESS, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Expose Prometheus /metrics endpoint (true/false)
PROMETHEUS_ENABLED=true

# --- Network ---

# Use gateway/worker subprocess architecture for lower idle memory (true/false)
GATEWAY_MODE=false

# Run on Unix domain socket instead of TCP (leave empty for TCP)
UNIX_SOCKET_PATH=

# TLS certificate paths for HTTP/2 (leave empty to disable)
SSL_KEYFILE=
SSL_CERTFILE=

# --- System (rarely need to change) ---

# Pin inference thread to specific CPU cores (e.g. "0-3,6"; empty = OS default)
INFERENCE_CPU_CORES=

# PyTorch CUDA memory allocator config
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# jemalloc preload for better memory allocation
LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libjemalloc.so.2
MALLOC_CONF=background_thread:true,dirty_decay_ms:1000,muzzy_decay_ms:0
